% Created 2021-01-06 Wed 10:53
% Intended LaTeX compiler: pdflatex
\documentclass[journal, oneside, twocolumn]{IEEEtran}
% \documentclass[12pt, draftclsenofoot, oneside, onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\graphicspath{./image/}
\DeclareGraphicsExtensions{.pdf, .jpeg, .png}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{stix}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% page-break permission for equation: 0 :yes  10000:no  other:yes but let latex find some other way
\interdisplaylinepenalty=2500

% fix hyperlink to invisible equation number
\makeatletter
\def\IEEElabelanchoreqn#1{\bgroup
\def\@currentlabel{\p@equation\theequation}\relax
\def\@currentHref{\@IEEEtheHrefequation}\label{#1}\relax
\Hy@raisedlink{\hyper@anchorstart{\@currentHref}}\relax
\Hy@raisedlink{\hyper@anchorend}\egroup}
\makeatother
\newcommand{\subnumberinglabel}[1]{\IEEEyesnumber
\IEEEyessubnumber*\IEEElabelanchoreqn{#1}}

% fix font behavior of IEEEeqnarray
\renewcommand{\theequationdis}{{\normalfont (\theequation)}} 
\renewcommand{\theIEEEsubequationdis}{{\normalfont (\theIEEEsubequation)}} 

% fix distance between f and ()
\usepackage{mleftright}
\mleftright

% fix ()pairs in different line same size
\newcommand{\sizecorr}[1]{\makebox[0cm]{\phantom{$\displaystyle #1$}}}

% argmin argmax
\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argminB}{argmin}
\DeclareMathOperator*{\argmaxA}{arg\,max}
\DeclareMathOperator*{\argmaxB}{argmax}

\DeclareMathOperator{\tr}{Tr}

% fix d in integration
\newcommand{\dd}{\mathop{}\!\mathrm{d}}

\date{}

\title{Spatial Spectrum Inference with Transformed Kernel Learning}
\hypersetup{
pdfauthor={Yi-Qun Xu},
pdftitle={Spatial Spectrum Inference with Deep Kernel Learning},
pdfkeywords={},
pdfsubject={},
pdfcreator={vscode}, 
pdflang={English}}

\begin{document}

\author{
  Yi-Qun~Xu,
  Daoxing~Guo,%~\IEEEmembership{Member,~IEEE,}
  Bangning~Zhang,%~\IEEEmembership{Life~Fellow,~IEEE}

  %\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant 61901502 and Research Project of NUDT under grant ZK18-02-11.\emph{(Corresponding author: Bangning~Zhang.)}}
  %\thanks{Y.-Q. Xu, B. Zhang, G. Ding, and D. Guo are with the College of Communications Engineering, Army Engineering University, Nanjing 210000, China (e-mail: yi-qun.xu@foxmai.com; AEU\_zbn@163.com; dr.guoru.ding@ieee.org; xyzgfg@sina.com).}
  %\thanks{Manuscript received April 19, 2020; revised August 26, 2020.}
}

%\markboth{}{}
%\IEEEspecialpapernotice{(Invited Paper)}

\maketitle


\begin{abstract}

\end{abstract}


\begin{IEEEkeywords}
  spectrum inference, Gaussian process, deep kernel learning,
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{C}{rowd-sourced} spectrum sensing (CSS), which is to recruit private mobile communication terminals to perform spectrum sensing, has received many research attentions \cite{Ding2014, Jin2018, Han2019, Hu2020, Amin2020} in recent years. Billions of mobile devices can guarantee geographical coverage, especially in the urban area. To make use of the big data collected by the CSS system, One of the most stirring application is for spectrum inference. Spectrum inference is a generalization of spectrum prediction from time dimension to space and frequency dimensions, which is an promising technique for capturing the relevant information of radio frequency from measurement data.\cite{Ding2018}. Radio environment map (REM)
The space dimension spectrum inference problem which is called radio environment map construction in many research papers,has different names in different research papers. The two most common ones are radio environment map (REM) construction \cite{Phillips2012, Pesko2014, Sato2017, Li2018, Katagiri2020}

Various aspects of REM were studied in \cite{Li2018}. They summarized the main construction methods of REM including kriging, nearest neighbor, inverse distance weighted, trend surface, thin plant spines, discrete smooth interpolation and joint tensor completion.

In \cite{Xu2021}, they proposed a Bayesian hierarchical model-based method for REM construction, which take the correlation of shadow fading into consideration. However, the premise assumption of their research is

data-driven method: accuracy is not good

spatial statistical method base on assumptions of heterogeneous of space field.
In this letter, we will focus on the space dimension of spectrum inference.

% !现有研究进展

% !概括本文内容
In this letter, we propose a deep kernel learning based spatial spectrum inference method for heterogeneous environment. Specifically, we model the shadow fading effect as an anisotropic and nonstationary Gaussian process. This model is flexible and consistent with the actual situation. And by leveraging the power of deep neural network, we improve the expressive ability of Gaussian process regression (GPR) model. To the best of our knowledge, this is the first attempt to introduce GPs to spatial spectrum inference.

The remainder of the letter is as follows: After the system model in Section II, the deep kernel learning method for radio environment map is considered in Section III. Subsequently, the simulation results and concluding remarks are expressed in Section IV and V.



\section{System Model and Problem Statement}

In this letter, we consider an area of 1km by 1km in 2D space $D^2$. Measurements of $N$ crowd-sourced sensors are reported to the fusion center with its location and receive signal strength (RSS).
From the propagation model, the RSS of the $j$-th sensor can be represented as:
\begin{IEEEeqnarray}{rCl}
  {P}_{r}^{(j)}[dBm] & = & P_t[dBm] - 20log_{10}\left(\frac{4 \pi d_0}{\lambda}\right) \IEEEnonumber\\
  & - & 10 \eta log_{10}{\left(\frac{\lVert L_{t} - l_{j} \rVert}{d_0}\right)} - S_{j}[dB] + N_j[dB],\IEEEeqnarraynumspace
  \label{equ:propagation_model}
\end{IEEEeqnarray}
where $P_t$ is the transmitted power,  $d_0$ is a reference distance, $\lambda$ is the wavelength of radiation, $\eta$ is the free-space path loss exponent, $L_t$ is the location of the transmitter, $l_j$ is the location of the $j$-th sensor, $S_{j}$ is the shadowing component from the transmitter to $j$-th sensor, and $N_j$ is the measurement error caused by crowd-source sensor. 

The main task for spatial inference is to obtain the RSS where there is no crowd-sourced sensor and further draw the REM of the area of interest. From Eq.~\eqref{equ:propagation_model}, the RSS can be decomposed into three parts: the free-path loss component, the shadowing component, and the measurement error. The free-path loss component can be calculated with the link budget. The difficulty for spatial inference is caused by the shadowing component $S_j$\cite{Xu2021}. In radio communication systems, shadowing is a random phenomenon, which is typically modeled by a log-normal distribution\cite{Cho2010}. Moreover, the spatial correlation of shadowing has been studied systematically\cite{Agrawal2009, Gudmundson1991}, and used in REM construction to improve the inference accuracy\cite{Han2019, Sato2017, Xu2021}. However, in previous studies, the shadowing component is treated as an isotropic stationary process. It's a strong assumption that simplifies reality dramatically. The shadowing is affected by the obstacles in the environment. In most cases, the obstacles are not evenly distributed. Therefore, due to the heterogeneity of the environment, the shadowing component should be modeled as an anisotropic and nonstationary process. This attribute of shadowing increases the challenge for spatial spectrum inference. 

Our study aims to characterize the heterogeneity of the environment to improve the accuracy of spatial spectrum inference.

\section{Transformed Kernel Learning}

In this section, we first introduce some background on the Gaussian process (GP) and deep kernel learning (DKL) and then elaborate proposed approach for spatial spectrum inference: transformed kernel learning (TKL).

\subsection{Background on Gaussian Process}
GP as a powerful tool in the machine learning toolbox \cite{Rasmussen2006} has been extensively studied in recent years\cite{Damianou2013, Wilson2013,Duvenaud2014a,Salimbeni2017a, Lee2018,Wilson2019}. GP is suitable for complex regression tasks due to its flexibility in incorporating prior knowledge. Furthermore, GP is a probabilistic approach that allowing us to incorporate the confidence of the prediction into the regression result.

A GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. For a regression problem $y=f(x)+ \epsilon$, where $x\in\mathbf{R}^D$ and  $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$ is independent identical distributed (IID) Gaussian measurement noise, GP method assume the unknown function $f$ follow a GP which is $\mathcal{GP}\left(\mu(\cdot), k(\cdot, \cdot)\right)$, where $\mu(\cdot)$ is the mean function and $k(\cdot, \cdot)$ is the covariance function which is also called the kernel function.

For training inputs $\mathbf{X}=[x_1, x_2, \dots, x_n]^ T$ and corresponding noisy training targets $\mathbf{y}=[y_1, y_2, \dots, y_n]^T$, to predict the function values $f_*$ at the test points $x_*$, the joint distribution of training points and predict targets can be represented as:
\begin{IEEEeqnarray}{c}
  \left[ \begin{matrix} \mathbf{y} \\ f_* \end{matrix} \right]  \sim
  \mathcal{N} \left(\left[\begin{matrix} \mu_{f} \\
      \mu_{*}\end{matrix} \right],
  \left[ \begin{matrix} K_{ff}+ \sigma_{n}^{2}I & K_{f*} \\ K_{*f} & K_{**}\end{matrix} \right]  \right),
\end{IEEEeqnarray}
where the mean vector and the covariance matrix can be computed with the mean and kernel function of the GP:
\begin{IEEEeqnarray}{rCl}
  \mu_i  & = & \mu(x_i), \\
  K_{ij} & = & k(x_i, x_j),
\end{IEEEeqnarray}

We obtain the predictive distribution at point $x_*$ as:
\begin{IEEEeqnarray}{rCl}
  p\left( f_* \middle| \mathbf{X}, \mathbf{y}, x_* \right) & = & \mathcal{N}\left( f_* \middle| \mu_*, \sigma_*^2 \right),
\end{IEEEeqnarray}
where:
\begin{IEEEeqnarray}{rCl}
  \mu_* & = & \mu(x_*) + k(x_*, \mathbf{X})\left[k(\mathbf{X}, \mathbf{X}) + \sigma_N^2 \mathbf{I})^{-1}(\mathbf{y} - \mu(X)\right], \IEEEeqnarraynumspace  \\*
  \sigma_*^2 & = & k(x_*, x_*) - k(x_*, \mathbf{X}) \left[k(\mathbf{X}, \mathbf{X}) + \sigma_N^2 \mathbf{I})^{-1}k(\mathbf{X}, x_*)\right], \IEEEeqnarraynumspace
\end{IEEEeqnarray}

In GP regression (GPR), the mean function $\mu(\cdot)$ is often assumed to be $\mathbf{0}$. The GP is trained by maximize the marginal likelihood with respect to GP parameters. The marginal likelihood is given by:
\begin{IEEEeqnarray}{rCl}
  p\left(\mathbf{y} | \mathbf{X}, \theta \right) & = & \int p\left( \mathbf{y} | f, \mathbf{X} \right) p(f | \mathbf{X}) \dd f  \IEEEnonumber \\*
  & = & \int \mathcal{N}\left(\mathbf{y} \middle| f(\mathbf{X}), \sigma_{n}^{2} \mathbf{I} \right) \mathcal{N}\left( f(\mathbf{X}) \middle| \mathbf{0}, \mathbf{K} \right) \dd f(\mathbf{X})   \IEEEnonumber \\*
  & = & \mathcal{N} ( \mathbf{y} | \mathbf{0}, \mathbf{K} + \sigma_n^{2} \mathbf{I} ),
\end{IEEEeqnarray}
where $\mathbf{K}$ is the kernel matrix with $\mathbf{K}_{ij} = k(x_i, x_j)$ , $\mathbf{X}$ are training inputs, $\mathbf{y}$ are training targets. Therefore, the negative log marginal likelihood (NLML) is:
\begin{IEEEeqnarray}{rCl}
  \IEEEyesnumber
  \mathcal{L}(\theta) & = & - \text{log} p(\mathbf{y} | \mathbf{X}, \theta) \IEEEnonumber \\
  & = & \frac{1}{2}\mathbf{y}^T \Sigma_\theta^{-1}\mathbf{y} + \frac{1}{2}\text{log}(\text{det}\Sigma_\theta) + \frac{n}{2}\text{log}2\pi,
  \label{eq:nlml}
\end{IEEEeqnarray}
where:
\begin{IEEEeqnarray}{rCl}
  \Sigma_\theta & := & \mathbf{K} + \sigma_n^2 \mathbf{I}
\end{IEEEeqnarray}

The GP training is formulated as an optimization problem:
\begin{IEEEeqnarray}{rCl}
  \IEEEyesnumber
  \hat{\theta} & = & \argminB_{\theta} {\mathcal{L}(\theta)}
\end{IEEEeqnarray}
which is a non-convex problem. In practice, the gradient-based optimization algorithms are used to find the optimum of NLML.

\subsection{Deep Kernel Learning}
The kernel function $k(\cdot, \cdot)$ of GPs encodes structural assumptions about the class of functions we wish to model, which corresponding to the ``inductive bias'' of the learning method. In this sense, the prior knowledge about the model can be embedded into GP through the kernel function. There are many base kernels such as linear kernel, RBF kernel, periodic kernel, Matérn kernel, and spectral mixture kernel\cite{Wilson2013}. Each kernel function corresponds to a different set of assumptions made about the function we wish to model \cite{Duvenaud2014a}. For example, the linear kernel implies a linear function, and the periodic kernel implies a function with repeating structure \cite{Wilson2013}. The design and choice of the kernel have a profound impact on the performance of the GPR model.

DKL proposed by Wilson \cite{Wilson2019} encapsulate the expressive power of deep architectures into GP. Specifically, deep kernel learning starts from a base kernel $k(x_i, x_j |  \mathbf{\theta})$ with parameters $\mathbf{\theta}$, and then transform the inputs $x$ with a deep neuron network $g(x;\mathbf{w})$ parameterized by weights $\mathbf{w}$:
\begin{IEEEeqnarray}{rCl}
  k(x_i, x_j | \mathbf{\theta}) & \rightarrow & k( g(x_i, \mathbf{w}), g(x_j, \mathbf{w}) |  \mathbf{\theta}, \mathbf{w})
\end{IEEEeqnarray}

The deep kernel increases the expressive ability of GP significantly. However, it has been proved that the DKL is not stable, as the GP will use the flexibility of the deep neuron network to try to correlate all input points \cite{Ober2021}. 


\subsection{Transformed Kernel Learning}
One way to introduce non-stationarity is to introduce an arbitrary non-linear transformation $f(x)$ which map the input $x$ in space $D$ to a new space $D^\prime$, and then use a stationary covariance function in space $D^\prime$\cite{Rasmussen2006}. 
Inspired by the DKL method, we designed th
To model a nonstationary and anisotropic covariance structure. A nonlinear injective function $f$ can be used to mapping the space $D$ which is nonstationary and  anisotropic to a new space $D1$. 
We composite few simply nonlinear injective mapping to increase the representability of kernels. For purpose, the hyper-parameter values are chosen during design, the weight parameters are to be learned during learning procdural.

\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/axial_transformation_unit.eps}
  \caption{Axial transformation unit. (a) Basis functions for ATU, a linear function and a bunch of shifted sigmoid functions, (b) Axial non-linear transformation of the first dimension, (c) Axial non-linear transformation of the second dimension.}
  \label{fig:axial_trans_unit}
\end{figure}


\subsubsection{Axial Transformation Unit (ATU)}

The example of ATU is demonstrated in Fig.~\ref{fig:axial_trans_unit}.
The ATU is carried out in 3 steps:
First, we normalize the input dimension which to be transformed to range [0, 1]. 
Second, we perform nonlinear transformation through a bunch of basis functions $\boldsymbol{\phi}(s_k)$. The first basis functions is a linear scaling $\phi_1(s_k) = w_1x$, and the rest basis functions are shifted sigmoid functions $\phi_{i+1}(s_k; \boldsymbol{\alpha}) = \frac{1}{1 + exp(-\alpha_{i1}(s_k - \alpha_{i2}))},(i=1,\dots, r)$,where $\alpha_{i1}$ determines the grad of the sigmoid functions $\alpha_{i2}$ determines the position of the sigmoid functions, and $r$ is the number of sigmod functions which can represent the resolution of local transformation. Third, we normalize the output to range [0, 1].
Due to the Strict monotonicity of the sigmoid functions, if we constrain the weight parameters to positive values the ATU is garanteed to be an injective nonlinear transformation $f_{a}:[0, 1] \rightarrow [0,1]$, which has some flexibility controlled by the weight parameters. We define the ATU which transform the $k$-th dimension as follows:
\begin{IEEEeqnarray}{rCl}
  f_{a}^{(k)}(\mathbf{s};\mathbf{w}_a^{(k)}, \boldsymbol{\alpha}) &=& (s_1^\prime,s_2^\prime)\\
  \noalign{\noindent and \vspace{2\jot}}
  s_n^\prime &=&
  \begin{cases}
    s_n, & \text{if } n\neq k,\\
    \frac{g(s_k)- \min(g(s_k))}{\max(g(s_k)) - \min(g(s_k))},& \text{if } n=k,
  \end{cases}
\end{IEEEeqnarray}
where:
\begin{IEEEeqnarray}{rrCl}
& g(s_k;\boldsymbol{\alpha}) &=& \mathbf{w}_{a}^{(k)T} \cdot \boldsymbol{\phi}(s_k;\boldsymbol{\alpha}), \label{eq:block1_case1}\\* 
  \smash{\left\{ \IEEEstrut[8\jot] \right.} & \phi_1(s_k; \boldsymbol{\alpha}) & = & s_k \label{eq:block1_case2} \\* 
  & \phi_{i+1}(s_k; \boldsymbol{\alpha}) & = & 1/(1 + e^{-\alpha_{i1}(s_k - \alpha_{i2})}), i=1,\dots, r, \label{eq:block1_case3} \IEEEeqnarraynumspace 
\end{IEEEeqnarray}
$\boldsymbol{\alpha}=[\alpha_{ij}]_{r\times2}$ is the super-parameter, 
$\mathbf{w}_{(k)}$ is the weight parameter,  for the axial transformation of the $k$-th dimension.  the weighted sum of sigmoid function 

\subsubsection{Radical Basis Function Transformation Unit (RBFTU)}
\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/rbf_unit.eps}
  \caption{Radical basis function transformation unit. The upper figures show the boundary of the transformation, the lower figures show the transformed space (a) One local RBFTU expand a local area in the space (b) The whole space can be transformed with multiple RBFTUs evenly distributed.}
  \label{fig:rbf_unit}
\end{figure}
A RBFTU is a local expansion/contraction of the original space. We define RBFTU as follows:
\begin{IEEEeqnarray}{rCl}
  f_{r}(\mathbf{s}; w_r, \beta,\boldsymbol{\gamma}) &=& \mathbf{s} + w_{r}(\mathbf{s} - \boldsymbol{\gamma})e^{-\beta\lVert\mathbf{s} - \boldsymbol{\gamma}\rVert}, 
\end{IEEEeqnarray}
% \begin{IEEEeqnarray}{rCl}
%   f(s_1) &=& s_1 + (s_1 - r_{ij}^{(1)})exp(-a\lVert\mathbf{s} - \mathbf{r}_{ij}\rVert), \\
%   f(s_2) &=& s_2 + (s_2 - r_{ij}^{(2)})exp(-a\lVert\mathbf{s} - \mathbf{r}_{ij}\rVert),
% \end{IEEEeqnarray}
where $\mathbf{s}=(s_1, s_2)$ is a point in the original space $D^2$, there are two super-parameters, $\boldsymbol{\gamma}=(\gamma_{1}, \gamma_{2})$ is the center of the local expansion/contraction, and $\beta$ controls the bolder of the local expansion/contraction, $w_r$ is the degree of expansion/contraction, the transformation can be guaranteed to be injective by constraining $w_r$ in $(-1, \frac{1}{2}e^{3/2})$. 

Panel (a) of Fig.~\ref{fig:rbf_unit} shows an example of RBFTU centered at location $(0.25, -0.125)$. By selecting different $\gamma$s and $\beta$s and cascading RBFTUs together, we can get a non-linear transformation of the whole space. Panel (b) of Fig.~\ref{fig:rbf_unit} shows a non-linear transformation of the space with RBFTUs evenly distributed in the space.
\subsubsection{M\"obius Transformation Unit (MTU)}
\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/mobius_unit.eps}
  \caption{M\"obius transformation unit.}
  \label{fig:mobius_unit}
\end{figure}
M\"obius trasformation is an one-to-one, onto and conformal mapping used in complex plane.[Geometry of MT].Let $z=s_1 + s_2 \mathbf{j}, \mathbf{j}=\sqrt{-1}$, the M\"obius transformation is defined as:
\begin{IEEEeqnarray}{rCl}
  f_{m}(\mathbf{s}; \mathbf{w}_m) &=& \left(\Re[g_m(z)], \Im[g_m(z)]\right)
\end{IEEEeqnarray}
where: 
\begin{IEEEeqnarray}{rCl}
  g_{m}(z) &=& \frac{az + b}{cz+d} 
\end{IEEEeqnarray}
$a,b,c,d$ are complex numbers, the weight parmeter of MTU is:
\begin{IEEEeqnarray}{rCl}
  \mathbf{w}_m = [\Re(a),\Im(a), \Re(b),\Im(b),\Re(c),\Im(c), \Re(d),\Im(d)]\IEEEeqnarraynumspace
\end{IEEEeqnarray}

The above three kinds of transformations are all injective transfromation and keep the homeomorphic. Compositions of them can keep the homeomorphism and provide a flexible representation of the spatial nonstationary. How to choose the composition of them and the hyper-parameters $\boldsymbol{\theta}=[\boldsymbol{\alpha}, \beta, \boldsymbol{\gamma}]$ is a design task. 
The weight parameters $\mathbf{W} = [\mathbf{w}_a^{(1)}, \mathbf{w}_a^{(2)}, w_r^{(1)}, \ldots, w_r^{(N)}, \mathbf{w}_m ]$ and parameters of the GP base kernel can be learned jointly by minimizing the NLML (see Eq.~\eqref{eq:nlml}) of GP. 
First, different from the transformation of NN, they can keep the topology structure of the space.  Second, It is not prone to overfit during the training procedual. However, the hyper-parameter is subttle.
\section{Numerical Results}

\begin{figure*}[!b]
  \centering
  \includegraphics[width=7.12in]{./images/gp_model.eps}
  \caption{GP model.}
  \label{fig:gp_model}
\end{figure*}

We composite the ATU of dimension 1, ATU of dimension 2, a sequence of RBFTUs d and a MTU to represent the spatial non-linear transformation of the original space. Resolutions of both ATUs are 10. RBFUTs are evenly distributed in space with resolution $27\times27$.We choose the practical default kernel in GP, the RBF kernel as the base kernel of the GP. There are  parameters: 11 for each ATU, 27 for RBFTUs, 8 for MTU, and 2 for base kernel (scale parameter and length parameter). The composition of transformation can be represented as:
\begin{IEEEeqnarray}{rCl}
  f(\cdot) = f_m\circ f_r^{(27)} \circ f_r^{(26)}\circ \cdots \circ f_r^{(1)}\circ f_a^{(2)} \circ f_a^{(1)}(\cdot)
\end{IEEEeqnarray}
Starting from the base kernel with parameter $\sigma$ and $l$,   
\begin{IEEEeqnarray}{rCl}
  k(\mathbf{s}_i, \mathbf{s}_j;\sigma, l) = \sigma^2\exp\left(-\frac{\lVert \mathbf{s}_i - \mathbf{s}_j\rVert}{l^2}\right)
\end{IEEEeqnarray}
We transform the input $\mathbf{s}$ as:
\begin{IEEEeqnarray}{rCl}
  k(\mathbf{s}_i, \mathbf{s}_j;\sigma, l) \rightarrow k(f(\mathbf{s}_i;\mathbf{w}, \mathbf{\theta}), f(\mathbf{s}_j;\mathbf{w}, \mathbf{\theta}); \sigma,l, \mathbf{w}, \mathbf{\theta})
\end{IEEEeqnarray}
where $f(s;\mathbf(W), \boldsymbol{\theta})$ is the composition non-linear mapping parameterized by $\mathbf{W}$. 
We find that the TKGPR method is not prone to overfitting.
We further compare TKGPR with NN, IWNN, OK, and normal GPR with RBF kernel. 


Here, we will evaluate the performance of the proposed algorithm with the CRAWDAD cu/WiMAX datasets \cite{Ton2012}, which was also used in \cite{Phillips2012} and \cite{Hu2020}. The cu/wimax datasets was collected at the University of Colorado Boulder. It contains careful point measurements of the Wimax network serving the campus. We investigate the ``first phase'' sample, which were taken on a 100m equilateral triangular lattice.





\section{Conclusion}
\subsection{}


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{./bibtex/myIEEEtran.bst}
\bibliography{./bibtex/library}

%------------------------------------------------------------------------------------------------
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Yi-Qun_Xu}}]{Yi-Qun~Xu}
%   received the B.S. and M.S. degrees in information and communication
%   engineering from the PLA University of Science and Technology, Nanjing, China,
%   in 2005 and 2010, respectively. He is currently pursuing the Ph.D. degree with
%   Army Engineering University, Nanjing, China.

%   His current research interests include cognitive radio, spectrum sensing,
%   spatial-temporal data analysis, big data analysis, and machine learning.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Bangning_Zhang}}]{Bangning~Zhang}
%   received the B.S. and M.S. degrees from the Institute of Communications Engineering, Nanjing, China, in 1984 and 1987, respectively. His current research interests include microwave technologies, satellite communication systems, communication anti-jamming technologies, and physical layer security.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Guoru_Ding}}]{Guoru~Ding}
%   (S'10-M'14-SM'16) is currently an Associate Professor with the College of Communications Engineering, Nanjing, China. He received the B.S. (Hons.) degree in electrical engineering from Xidian University, Xi'an, China, in 2008, and the Ph.D. (Hons.) degree in communications and information systems from the College of Communications Engineering, Nanjing, China, in 2014. From 2015 to 2018, he was a Post-Doctoral Research Associate with the National Mobile Communications Research Laboratory, Southeast University, Nanjing, China. His research interests include cognitive radio networks, massive MIMO, machine learning, and data analytics over wireless networks.

%   He has received the Excellent Doctoral Thesis Award of the China Institute of Communications in 2016, the Alexander von Humboldt Fellowship in 2017, the Excellent Young Scientist of Wuwenjun Artificial Intelligence in 2018, and the 14th IEEE COMSOC Asia-Pacific Outstanding Young Researcher Award in 2019. He was a recipient of the Natural Science Foundation for Distinguished Young Scholars of Jiangsu Province, China and six best paper awards from international conferences such as the IEEE VTC-FALL 2014. He has served as a Guest Editor for the IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS (special issue on spectrum sharing and aggregation in future wireless networks). He is currently an Associate Editor of the IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, an Editor of CHINA COMMUNICATIONS and a Technical Editor of the IEEE 1900.6 Standard Association Working Group.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Bing_Zhao}}]{Bing~Zhao}
%   received the B.S. degree and M.S. degree from Nanjing University of Science and Technology (NJUST), Nanjing, China, in 2007 and 2009, respectively. She is currently a full lecturer with Army Engineering University of PLA. She has been granted over 10 patents in her research areas. Her current research interests include satellite communications systems and transmission technologies.
%   \end{IEEEbiography}

%   \begin{IEEEbiographynophoto}{Shengnan~Li}
%   received the B.S. degree from the Changsha University of Science and Technology, Changsha, China, in 2014, and the M.S. degree from the PLA University of Science and Technology, Nanjing, in 2017. Her research interests focus on spectrum sensing, satellite communications, and communication anti-jamming technologies.
%   \end{IEEEbiographynophoto}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Daoxing_Guo}}]{Daoxing~Guo}
%   received the B.S., M.S., and Ph.D. degrees from the Institute of Communications Engineering, Nanjing, China, in 1995, 1999, and 2002, respectively. His current research interests include satellite communication systems and transmission technologies, communication anti-jamming technologies, and communication anti-interception technologies.
%   \end{IEEEbiography}
\end{document}