% Created 2021-01-06 Wed 10:53
% Intended LaTeX compiler: pdflatex
\documentclass[journal, oneside, twocolumn]{IEEEtran}
% \documentclass[12pt, draftclsenofoot, oneside, onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\graphicspath{./image/}
\DeclareGraphicsExtensions{.pdf, .jpeg, .png}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{stix}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% page-break permission for equation: 0 :yes  10000:no  other:yes but let latex find some other way
\interdisplaylinepenalty=2500

% fix hyperlink to invisible equation number
\makeatletter
\def\IEEElabelanchoreqn#1{\bgroup
\def\@currentlabel{\p@equation\theequation}\relax
\def\@currentHref{\@IEEEtheHrefequation}\label{#1}\relax
\Hy@raisedlink{\hyper@anchorstart{\@currentHref}}\relax
\Hy@raisedlink{\hyper@anchorend}\egroup}
\makeatother
\newcommand{\subnumberinglabel}[1]{\IEEEyesnumber
\IEEEyessubnumber*\IEEElabelanchoreqn{#1}}

% fix font behavior of IEEEeqnarray
\renewcommand{\theequationdis}{{\normalfont (\theequation)}} 
\renewcommand{\theIEEEsubequationdis}{{\normalfont (\theIEEEsubequation)}} 

% fix distance between f and ()
\usepackage{mleftright}
\mleftright

% fix ()pairs in different line same size
\newcommand{\sizecorr}[1]{\makebox[0cm]{\phantom{$\displaystyle #1$}}}

% argmin argmax
\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argminB}{argmin}
\DeclareMathOperator*{\argmaxA}{arg\,max}
\DeclareMathOperator*{\argmaxB}{argmax}

\DeclareMathOperator{\tr}{Tr}

% fix d in integration
\newcommand{\dd}{\mathop{}\!\mathrm{d}}

\date{}

\title{Spatial Spectrum Inference with Transformed Kernel Learning}
\hypersetup{
pdfauthor={Yi-Qun Xu},
pdftitle={Spatial Spectrum Inference with Deep Kernel Learning},
pdfkeywords={},
pdfsubject={},
pdfcreator={vscode}, 
pdflang={English}}

\begin{document}

\author{
  Yi-Qun~Xu,
  Daoxing~Guo,%~\IEEEmembership{Member,~IEEE,}
  Bangning~Zhang,%~\IEEEmembership{Life~Fellow,~IEEE}

  %\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant 61901502 and Research Project of NUDT under grant ZK18-02-11.\emph{(Corresponding author: Bangning~Zhang.)}}
  %\thanks{Y.-Q. Xu, B. Zhang, G. Ding, and D. Guo are with the College of Communications Engineering, Army Engineering University, Nanjing 210000, China (e-mail: yi-qun.xu@foxmai.com; AEU\_zbn@163.com; dr.guoru.ding@ieee.org; xyzgfg@sina.com).}
  %\thanks{Manuscript received April 19, 2020; revised August 26, 2020.}
}

%\markboth{}{}
%\IEEEspecialpapernotice{(Invited Paper)}

\maketitle


\begin{abstract}
data-driven method: accuracy is not goodIn this letter, we focus on the space dimension of spectruminference.In this letter, we propose a deep kernel learning-basedspatial spectrum inference method for heterogeneous envi-ronments. Specifically, we model the shadow fading effectas an anisotropic and nonstationary Gaussian process. Thismodel is flexible and consistent with the actual situation.And by leveraging the power of the deep neural network,we improve the expressive ability of the Gaussian processregression (GPR) model. To the best of our knowledge, this isthe first attempt to introduce GPs to spatial spectrum inference.
\end{abstract}


\begin{IEEEkeywords}
  spectrum inference, Gaussian process, deep kernel learning,
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{C}{rowd-sourced} spectrum sensing (CSS), which is to recruit private mobile communication terminals to perform spectrum sensing, has received much research attentions \cite{Ding2014, Jin2018, Han2019, Hu2020, Amin2020} in recent years. Billions of mobile devices can guarantee geographical coverage, especially in urban area. To make use of the big data collected by the CSS system, one of the most stirring application is for spectrum inference. Spectrum inference is a generalization of spectrum prediction from time dimension to space and frequency dimensions, which is a promising technique for capturing the relevant information of radio frequency from measurement data.\cite{Ding2018}.

The spatial spectrum inference (SSI) is to get the receive signal strength (RSS), where there is no sensor device, from data collected by spatially distributed sensors. It is regarded as radio environment map (REM) construction method in many research papers\cite{Phillips2012, Pesko2014, Sato2017, Li2018, Han2019, Katagiri2020, Xu2021}. Various aspects of REM were studied in \cite{Li2018}. They summarized the main state-of-art construction methods of REM including Kriging, nearest neighbor (NN), inverse distance weighted nearest neighbor (IDWNN), trend surface, thin plant spines, discrete smooth interpolation and joint tensor completion. In \cite{Han2019}, the authors made a comparison between NN, IDWNN and the Kriging method. The Kriging algorithm has the best accuracy. In \cite{Xu2021}, they proposed a Bayesian hierarchical model-based method for REM construction, which take the correlation of shadow fading into consideration. 

Recent researches show the potential of spatial statistics methods in the problem of SSI. However, another powerful tool in spatial statistics, the Gaussian process (GP), has never received attention it deserves in SSI. Essentially, SSI is a complex regression task. GP as a powerful tool in the machine learning toolbox \cite{Rasmussen2006} has been extensively studied in recent years\cite{Damianou2013, Wilson2013,Duvenaud2014a,Salimbeni2017a, Lee2018,Wilson2019}. It is suitable for complex regression tasks due to its flexibility in incorporating prior knowledge. Furthermore, GP is a probabilistic approach that allowing us to incorporate the confidence of the prediction into the regression result.

In this letter, we we propose a transformed kernel learning (TKL) method for SSI based on GP. for heterogeneous environments. Specifically, we model the shadow fading effect as an anisotropic and nonstationary Gaussian process. This model is flexible and consistent with the actual situation. And by leveraging the power of the deep neural network, we improve the expressive ability of the Gaussian process regression (GPR) model. To the best of our knowledge, this is the first attempt to introduce GPs to spatial spectrum inference.
We then show a considerable improvement in RMSE and CRPS compared to existing state-of-the-art SSI methods.

The remainder of the letter is as follows: After the system model in Section~II, the deep kernel learning method for radio environment map is considered in Section~III. Subsequently, the simulation results and concluding remarks are expressed in Section~IV and Section~V.



\section{System Model and Problem Statement}

In this letter, we consider an area of 1km by 1km in 2D space $D^2$. Measurements of $N$ crowd-sourced sensors are reported to the fusion center with its location and receive signal strength (RSS).
From the propagation model, the RSS of the $j$-th sensor can be represented as:
\begin{IEEEeqnarray}{rCl}
  {P}_{r}^{(j)}[dBm] & = & P_t[dBm] - 20log_{10}\left(\frac{4 \pi d_0}{\lambda}\right) \IEEEnonumber\\
  & - & 10 \eta log_{10}{\left(\frac{\lVert L_{t} - l_{j} \rVert}{d_0}\right)} - S_{j}[dB] + N_j[dB],\IEEEeqnarraynumspace
  \label{eq:propagation_model}
\end{IEEEeqnarray}
where $P_t$ is the transmitted power,  $d_0$ is a reference distance, $\lambda$ is the wavelength of radiation, $\eta$ is the free-space path loss exponent, $L_t$ is the location of the transmitter, $l_j$ is the location of the $j$-th sensor, $S_{j}$ is the shadowing component from the transmitter to $j$-th sensor, and $N_j$ is the measurement error caused by crowd-source sensor. 

The main task for spatial inference is to obtain the RSS where there is no crowd-sourced sensor and further draw the REM of the area of interest. From Eq.~\eqref{eq:propagation_model}, the RSS can be decomposed into three parts: the free-path loss component, the shadowing component, and the measurement error. The free-path loss component can be calculated with the link budget. The difficulty for spatial inference is caused by the shadowing component $S_j$\cite{Xu2021}. In radio communication systems, shadowing is a random phenomenon, which is typically modeled by a log-normal distribution\cite{Cho2010}. Moreover, the spatial correlation of shadowing has been studied systematically\cite{Agrawal2009, Gudmundson1991} and used in REM construction to improve the inference accuracy\cite{Han2019, Sato2017, Xu2021}. However, in previous studies, the shadowing component is treated as an isotropic stationary process. It's a strong assumption that simplifies reality dramatically. The shadowing is affected by the obstacles in the environment. In most cases, the obstacles are not evenly distributed. Therefore, due to the heterogeneity of the environment, the shadowing component should be modeled as an anisotropic and nonstationary process. This attribute of shadowing increases the challenge for spatial spectrum inference. 

Our study aims to characterize the heterogeneity of the environment to improve the accuracy of spatial spectrum inference.

\section{Transformed Kernel Learning}

In this section, we first introduce some background on the Gaussian process (GP) and deep kernel learning (DKL) and then elaborate proposed approach for spatial spectrum inference: transformed kernel learning (TKL).

\subsection{Background on Gaussian Process}

A GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. For a regression problem $y=f(x)+ \epsilon$, where $x\in\mathbf{R}^D$ and  $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$ is independent identical distributed (IID) Gaussian measurement noise, GP method assume the unknown function $f$ follow a GP which is $\mathcal{GP}\left(\mu(\cdot), k(\cdot, \cdot)\right)$, where $\mu(\cdot)$ is the mean function and $k(\cdot, \cdot)$ is the covariance function which is also called the kernel function.

For training inputs $\mathbf{X}=[x_1, x_2, \dots, x_n]^ T$ and corresponding noisy training targets $\mathbf{y}=[y_1, y_2, \dots, y_n]^T$, to predict the function values $f_*$ at the test points $x_*$, the joint distribution of training points and predict targets can be represented as:
\begin{IEEEeqnarray}{c}
  \left[ \begin{matrix} \mathbf{y} \\ f_* \end{matrix} \right]  \sim
  \mathcal{N} \left(\left[\begin{matrix} \mu_{f} \\
      \mu_{*}\end{matrix} \right],
  \left[ \begin{matrix} K_{ff}+ \sigma_{n}^{2}I & K_{f*} \\ K_{*f} & K_{**}\end{matrix} \right]  \right),
\end{IEEEeqnarray}
where the mean vector and the covariance matrix can be computed with the mean and kernel function of the GP:
\begin{IEEEeqnarray}{rCl}
  \mu_i  & = & \mu(x_i), \\
  K_{ij} & = & k(x_i, x_j),
\end{IEEEeqnarray}

We obtain the predictive distribution at point $x_*$ as:
\begin{IEEEeqnarray}{rCl}
  p\left( f_* \middle| \mathbf{X}, \mathbf{y}, x_* \right) & = & \mathcal{N}\left( f_* \middle| \mu_*, \sigma_*^2 \right),
\end{IEEEeqnarray}
where:
\begin{IEEEeqnarray}{rCl}
  \mu_* & = & \mu(x_*) + k(x_*, \mathbf{X})\left[k(\mathbf{X}, \mathbf{X}) + \sigma_N^2 \mathbf{I})^{-1}(\mathbf{y} - \mu(X)\right], \IEEEeqnarraynumspace  \\*
  \sigma_*^2 & = & k(x_*, x_*) - k(x_*, \mathbf{X}) \left[k(\mathbf{X}, \mathbf{X}) + \sigma_N^2 \mathbf{I})^{-1}k(\mathbf{X}, x_*)\right], \IEEEeqnarraynumspace
\end{IEEEeqnarray}

In GP regression (GPR), the mean function $\mu(\cdot)$ is often assumed to be $\mathbf{0}$. The GP is trained by maximize the marginal likelihood with respect to GP parameters. The marginal likelihood is given by:
\begin{IEEEeqnarray}{rCl}
  p\left(\mathbf{y} | \mathbf{X}, \theta \right) & = & \int p\left( \mathbf{y} | f, \mathbf{X} \right) p(f | \mathbf{X}) \dd f  \IEEEnonumber \\*
  & = & \int \mathcal{N}\left(\mathbf{y} \middle| f(\mathbf{X}), \sigma_{n}^{2} \mathbf{I} \right) \mathcal{N}\left( f(\mathbf{X}) \middle| \mathbf{0}, \mathbf{K} \right) \dd f(\mathbf{X})   \IEEEnonumber \\*
  & = & \mathcal{N} ( \mathbf{y} | \mathbf{0}, \mathbf{K} + \sigma_n^{2} \mathbf{I} ),
\end{IEEEeqnarray}
where $\mathbf{K}$ is the kernel matrix with $\mathbf{K}_{ij} = k(x_i, x_j)$ , $\mathbf{X}$ are training inputs, $\mathbf{y}$ are training targets. Therefore, the negative log marginal likelihood (NLML) is:
\begin{IEEEeqnarray}{rCl}
  \IEEEyesnumber
  \mathcal{L}(\theta) & = & - \text{log} p(\mathbf{y} | \mathbf{X}, \theta) \IEEEnonumber \\
  & = & \frac{1}{2}\mathbf{y}^T \Sigma_\theta^{-1}\mathbf{y} + \frac{1}{2}\text{log}(\text{det}\Sigma_\theta) + \frac{n}{2}\text{log}2\pi,
  \label{eq:nlml}
\end{IEEEeqnarray}
where:
\begin{IEEEeqnarray}{rCl}
  \Sigma_\theta & := & \mathbf{K} + \sigma_n^2 \mathbf{I}
  \label{eq:kernel_plus_i}
\end{IEEEeqnarray}

The GP training is formulated as an optimization problem:
\begin{IEEEeqnarray}{rCl}
  \IEEEyesnumber
  \hat{\theta} & = & \argminB_{\theta} {\mathcal{L}(\theta)}
\end{IEEEeqnarray}
which is a non-convex problem. In practice, the gradient-based optimization algorithms are used to find the optimum of NLML.

\subsection{Deep Kernel Learning}
The kernel function $k(\cdot, \cdot)$ of GPs encodes structural assumptions about the class of functions we wish to model, which corresponding to the ``inductive bias'' of the learning method. In this sense, the prior knowledge about the model can be embedded into GP through the kernel function. There are many base kernels such as linear kernel, RBF kernel, periodic kernel, Matérn kernel, and spectral mixture kernel\cite{Wilson2013}. Each kernel function corresponds to a different set of assumptions made about the function we wish to model \cite{Duvenaud2014a}. For example, the linear kernel implies a linear function, and the periodic kernel implies a function with repeating structure \cite{Wilson2013}. The design and choice of the kernel have a profound impact on the performance of the GPR model.

DKL proposed by Wilson \cite{Wilson2019} encapsulate the expressive power of deep architectures into GP. Specifically, deep kernel learning starts from a base kernel $k(x_i, x_j |  \mathbf{\theta})$ with parameters $\mathbf{\theta}$, and then transform the inputs $x$ with a deep neural network (DNN) $g(x;\mathbf{w})$ parameterized by weights $\mathbf{w}$:
\begin{IEEEeqnarray}{rCl}
  k(x_i, x_j | \mathbf{\theta}) & \rightarrow & k( g(x_i, \mathbf{w}), g(x_j, \mathbf{w}) |  \mathbf{\theta}, \mathbf{w})
\end{IEEEeqnarray}
The deep kernel increases the expressive ability of GP significantly. DKL views the resulting kernel as a scalable deep kernel and provides the ability to automatic learning of the kernel through GP marginal likelihood. 

However, it has been proved that, in some cases, the DKL is not stable, as the GP will use the flexibility of the DNN to try to correlate all inputs. Meanwhile, the DNN in DKL is prone to overfitting\cite{Ober2021}. 


\subsection{Transformed Kernel Learning}
One way to introduce nonstationarity in GP is to introduce an arbitrary non-linear transformation $f(x)$ which map the input $x$ in space $D$ to a new space $D^\prime$, and then use a stationary covariance function in space $D^\prime$\cite{Rasmussen2006}. In DKL, the DNN can be viewed as a nonlinear function that transforms the inputs. And the transformation is characterized by the weights parameter of the DNN. However, the inputs of spatial spectrum inference are the locations of crowd-sourced sensors. The input space is the actual physical space. DNN transformation of the input space causes the spatial folding effect, which is inconsistent with the meaning of the physical quantity studied. Therefore, the spatial folding effect and the pitfalls of DKL make it unsuitable for spatial spectrum inference.

Inspired by the pros and cons of DKL, we should design a nonlinear transformation with the following properties:
\begin{itemize}
  \item The nonlinear transformation can be characterized by several parameters, which give it the flexibility to represent the anisotropy and nonstationarity.
  \item The nonlinear transformation is an injective mapping and topological homeomorphous, which does not cause spatial folding effects.
\end{itemize} 

We design 3 kinds of simple nonlinear transformations. Each of them conforms to both properties above. By compositing these transformation units, we can construct a flexible nonlinear transformation to characterize the heterogeneity of the space of interest. 

In the following, a detailed illustration of the transformation unit will be provided. For clarity, the hyper-parameters, which are represented with Greek alphabets $\alpha, \beta, \text{and } \gamma$, are chosen during design. The weight parameters, which are represented with $w$ and subscripts, are to be learned during model training procedural.

\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/axial_transformation_unit.eps}
  \caption{Axial transformation unit. (a) Basis functions for ATU, a linear function and a bunch of shifted sigmoid functions, (b) Axial non-linear transformation of the first dimension, (c) Axial non-linear transformation of the second dimension.}
  \label{fig:axial_trans_unit}
\end{figure}


\subsubsection{Axial Transformation Unit (ATU)}
An example of ATU is demonstrated in Fig.~\ref{fig:axial_trans_unit}.
The ATU is carried out in 3 steps:
First, we normalize the input dimension to be transformed to the range [0, 1]. 
Second, we perform nonlinear transformation through weighted sum of a bunch of basis functions $\boldsymbol{\phi}(s_k)$. The first basis functions is a linear scaling $\phi_1(s_k) = w_1x$, and the rest basis functions are shifted sigmoid functions $\phi_{i+1}(s_k; \boldsymbol{\alpha}) = \frac{1}{1 + exp(-\alpha_{i1}(s_k - \alpha_{i2}))},(i=1,\dots, r)$, where $\alpha_{i1}$ determines the gradient of the sigmoid functions, $\alpha_{i2}$ determines the position of the sigmoid functions, and $r$ is the number of sigmoid functions which can represent the resolution of local transformations. Third, we normalize the output to range [0, 1].

Due to the strict monotonicity of the sigmoid functions, if we constrain the weight parameters to positive values the ATU is guaranteed to be an injective nonlinear transformation $f_{a}:[0, 1] \rightarrow [0,1]$, which has some flexibility controlled by the weight parameters. We define the ATU which transform the $k$-th dimension as follows:
\begin{IEEEeqnarray}{rCl}
  f_{a}^{(k)}(\mathbf{s};\mathbf{w}_a^{(k)}, \boldsymbol{\alpha}) &=& (s_1^\prime,s_2^\prime),\\
  \noalign{\noindent and \vspace{2\jot}}
  s_n^\prime &=&
  \begin{cases}
    s_n, & \text{if } n\neq k,\\
    \frac{g(s_k)- \min(g(s_k))}{\max(g(s_k)) - \min(g(s_k))},& \text{if } n=k,
  \end{cases}
\end{IEEEeqnarray}
where
\begin{IEEEeqnarray}{rrCl}
& g(s_k;\boldsymbol{\alpha}) &=& \mathbf{w}_{a}^{(k)T} \cdot \boldsymbol{\phi}(s_k;\boldsymbol{\alpha}), \label{eq:block1_case1}\\* 
  \smash{\left\{ \IEEEstrut[8\jot] \right.} & \phi_1(s_k; \boldsymbol{\alpha}) & = & s_k \label{eq:block1_case2} \\* 
  & \phi_{i+1}(s_k; \boldsymbol{\alpha}) & = & 1/(1 + e^{-\alpha_{i1}(s_k - \alpha_{i2})}), i=1,\dots, r, \label{eq:block1_case3} \IEEEeqnarraynumspace 
\end{IEEEeqnarray}
$\boldsymbol{\alpha}=[\alpha_{ij}]_{r\times2}$ is the hyper-parameter, $\mathbf{w}_{(k)}$ is the weight parameter for the axial transformation of the $k$-th dimension..

\subsubsection{Radial Basis Function Transformation Unit (RBFTU)}
\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/rbf_unit.eps}
  \caption{Radical basis function transformation unit. The upper figures show the boundary of the transformation, the lower figures show the transformed space (a) One local RBFTU expand a local area in the space (b) The whole space can be transformed with multiple RBFTUs evenly distributed.}
  \label{fig:rbf_unit}
\end{figure}
A RBFTU is a local expansion/contraction of the original space. We define RBFTU as follows:
\begin{IEEEeqnarray}{rCl}
  f_{r}(\mathbf{s}; w_r, \beta,\boldsymbol{\gamma}) &=& \mathbf{s} + w_{r}(\mathbf{s} - \boldsymbol{\gamma})e^{-\beta\lVert\mathbf{s} - \boldsymbol{\gamma}\rVert}, 
\end{IEEEeqnarray}
where $\mathbf{s}=(s_1, s_2)$ is a point in the original space $D^2$. There are two hyper-parameters: $\boldsymbol{\gamma}=(\gamma_{1}, \gamma_{2})$ is the center of the local expansion/contraction, and $\beta$ controls the bolder of the local expansion/contraction. The weight parameter $w_r$ controls the degree of expansion/contraction. The transformation can be guaranteed to be injective by constraining $w_r$ in $(-1, \frac{1}{2}e^{3/2})$\cite{Perrin1999}. 

Panel (a) of Fig.~\ref{fig:rbf_unit} shows an example of RBFTU centered at location $(0.25, -0.125)$. By selecting different $\gamma$s and $\beta$s and cascading RBFTUs together, we can get a non-linear transformation of the whole space. Panel (b) of Fig.~\ref{fig:rbf_unit} shows a non-linear transformation of the space with RBFTUs evenly distributed in the space.
\subsubsection{M\"obius Transformation Unit (MTU)}
\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/mobius_unit.eps}
  \caption{M\"obius transformation unit. M\"obius transformation of a regular chessboard pattern on $D^2 =[0,1] \times [0,1]$ by randomly generate weight parameter in $\mathbf{w}_m$ from standard normal distribution subject to the constrain that $-d/c$ is not in the area enclosed by points $[0+0\mathbf{j},0+1\mathbf{j},1+1\mathbf{j},1+0\mathbf{j}]$ in complex plane.}
  \label{fig:mobius_unit}
\end{figure}

M\"obius transformation is an one-to-one, onto and conformal mapping used in complex plane\cite{Olsen2010}. Let $z=s_1 + s_2 \mathbf{j}, (\mathbf{j}^2=-1)$, the MTU is defined as:
\begin{IEEEeqnarray}{rCl}
  f_{m}(\mathbf{s}; \mathbf{w}_m) &=& \left(\Re[g_m(z)], \Im[g_m(z)]\right),
\end{IEEEeqnarray}
where 
\begin{IEEEeqnarray}{rCl}
  g_{m}(z) &=& \frac{az + b}{cz+d},
  \label{eq:mobius_transform}
\end{IEEEeqnarray}
$a,b,c,d$ are complex numbers satisfying $ad-bc\neq 0 $, the weight parameters of MTU are:
\begin{IEEEeqnarray}{rCl}
  \mathbf{w}_m = [\Re(a),\Im(a), \Re(b),\Im(b),\Re(c),\Im(c), \Re(d),\Im(d)].\IEEEeqnarraynumspace
\end{IEEEeqnarray}

Fig.~\ref{fig:mobius_unit} gives an example of MTU transforms a regular chessboard pattern in $D^2=[0, 1] \times [0, 1]$. From Eq.~\eqref{eq:mobius_transform}, we can deduce $g_m(-d/c)= \infty$. Therefore, if we normalize the area of interest into $D^2=[0, 1] \times [0, 1]$, to avoid mapping input point to $\infty$, $-d/c$ must be constrained not in area enclosed by points $[0+0\mathbf{j},0+1\mathbf{j},1+1\mathbf{j},1+0\mathbf{j}]$ in complex plane.


These three kinds of basic transformation units are all satisfying the two properties mentioned above. They can be used as building blocks for nonlinear transformation.
For specific scenarios, We can choose appropriate composition of them and set proper values for the hyper-parameters $[\boldsymbol{\alpha}, \beta, \boldsymbol{\gamma}]$. This is a design choice. Meanwhile, the weight parameters:
\begin{IEEEeqnarray}{rCl}
  \mathbf{W} = [\mathbf{w}_a^{(1)}, \mathbf{w}_a^{(2)}, w_r^{(1)}, \ldots, w_r^{(N)}, \mathbf{w}_m ]
\end{IEEEeqnarray}
and parameters of the GP base kernel can be learned jointly from data by minimizing the NLML (see Eq.~\eqref{eq:nlml}) of GP. The weight parameters improves the the expressive ability of GP and enables it to model spatial heterogeneity.

\section{Numerical Results}
\subsection{Model Construction}

\begin{figure}[!tb]
  \centering
  \includegraphics[width=3.3in]{./images/gp_model.eps}
  \caption{Transformed kernel Gaussian process regression (TKGPR) model.}
  \label{fig:tkgpr_model}
\end{figure}
As shown in Fig.~\ref{fig:tkgpr_model}, we construct a transformed kernel GPR (TKGPR) model for the SSI problem. The TKGPR is consists of an ATU of dimension 1, an ATU of dimension 2, a sequence of RBFTUs, and an MTU to represent the nonlinear transformation of the original space. Then the transformed inputs are fed into a GP. 

The hyper-parameters of TKGPR are chosen as follows: Resolutions of both ATUs are 20. RBFTUs are evenly distributed in space with resolution $27\times27$. The border of each RBFTU intersects the center of adjacent RBFTUs to ensure coverage of the entire area. We choose the RBF kernel, which is the practical default kernel in GP, as the base kernel of the GP. There are 128 parameters: 11 for each ATU, 27 for RBFTUs, 8 for MTU, and 2 for the base kernel (scale parameter and length parameter). 

The composition of transformation can be represented as:
\begin{IEEEeqnarray}{rCl}
  f(\cdot) = f_m\circ f_r^{(27)} \circ f_r^{(26)}\circ \cdots \circ f_r^{(1)}\circ f_a^{(2)} \circ f_a^{(1)}(\cdot).
\end{IEEEeqnarray}
Starting from the base kernel with parameter $\sigma$ and $l$,   
\begin{IEEEeqnarray}{rCl}
  k(\mathbf{s}_i, \mathbf{s}_j;\sigma, l) = \sigma^2\exp\left(-\frac{\lVert \mathbf{s}_i - \mathbf{s}_j\rVert}{2l^2}\right),
\end{IEEEeqnarray}
we transform the input $\mathbf{s}$ as:
\begin{IEEEeqnarray}{rCl}
  k(\mathbf{s}_i, \mathbf{s}_j;\sigma, l) \rightarrow k(f(\mathbf{s}_i;\mathbf{W}), f(\mathbf{s}_j;\mathbf{W}); \sigma,l, \mathbf{W})
  \label{eq:trans_kernel}
\end{IEEEeqnarray}
where $f(\mathbf{s};\mathbf{W})$ is the composition non-linear mapping parameterized by $\mathbf{W}$. 

\subsection{Training and Inference Procedure}
The training datasets of the GPR model are $\{\mathbf{X}, \mathbf{y}\}$, $\mathbf{X}$ are locations of crowd-sourced sensors, $\mathbf{y}$ are the RSSs. We use back-propagation and gradient descent algorithm to train the TKGPR model. The loss function is the NLML. From Eq.~\eqref{eq:nlml},\eqref{eq:kernel_plus_i}, and \eqref{eq:trans_kernel}, the NLML of TKGPR model can be represented as:
\begin{IEEEeqnarray}{rCl}
  \mathcal{L}(\mathbf{W}, \sigma^2, l, \sigma_n^2) & = & - \text{log} p(\mathbf{y} | \mathbf{X}, \mathbf{W}, \sigma^2, l, \sigma_n^2) \IEEEnonumber \\
  & = & \frac{1}{2}\mathbf{y}^T \left(K(\mathbf{W},\sigma^2,l) + \sigma_n^2 \mathbf{I} \right)^{-1}\mathbf{y}\IEEEnonumber \\
  & & + \frac{1}{2}\text{log}\left(\text{det}\left(K(\mathbf{W},\sigma^2,l) + \sigma_n^2 \mathbf{I} \right)\right) \IEEEnonumber\\
  & &+ \frac{n}{2}\text{log}2\pi,
  \label{eq:tkgpr_nlml}
\end{IEEEeqnarray}

The gradient of parameters can be computed using the chain rule:
\begin{IEEEeqnarray}{rCl}
  \frac{\partial \mathcal{L}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} =\frac{\partial \mathcal{L}(\boldsymbol{\theta})}{\partial \mathbf{K}_{\boldsymbol{\theta}}} \frac{\partial \mathbf{K}_{\boldsymbol{\theta}}}{\partial \boldsymbol{\theta}}, (\boldsymbol{\theta} = [\mathbf{W}, \sigma^2, l]),
\end{IEEEeqnarray}

% Thanks to the development of deep learning technologies in recent years. There are several highly efficient and modular implementation of GPs, such as ``GPflow''\cite{vanderWilk2020a} and ``GPyTorch''\cite{Gardner2019}. They have made the implementation of GP very convenient.

\subsection{Inference Performance Comparison}

\begin{figure*}[!tb]
  \centering
  \includegraphics[width=7.12in]{./images/radio_map.eps}
  \caption{Shadowing component of the RSS. (a) The (b)NN algorithm with 20 neighbours}
  \label{fig:radio_map}
\end{figure*}
We assessed the performance of NN, IDWNN, OK, GPR, and the proposed TKGPR method with different numbers of crowd-sourced sensors in an area of 1km by 1km. Fig.~\ref{fig:radio_map} show the true shadowing component and shadowing component inferenced with 5 different algorithms from the crowd-sourced data. They give us an intuitive sense of the performance of different algorithms.

In all cases, SSI performance was assessed by evaluating on a grid of $180\times180$ points covering the area of interest. The performance metrics include the the root mean squared error (RMSE), the continuous ranked probability score (CRPS),
and the interval score of the 95\% prediction interval (IS)\cite{Gneiting2007}. 

their predictive performance to those of various models.

We find that the TKGPR method is not prone to overfitting.
We further compare TKGPR with NN, IWNN, OK, and normal GPR with RBF kernel. 


% Here, we will evaluate the performance of the proposed algorithm with the CRAWDAD cu/WiMAX datasets \cite{Ton2012}, which was also used in \cite{Phillips2012} and \cite{Hu2020}. The cu/wimax datasets was collected at the University of Colorado Boulder. It contains careful point measurements of the Wimax network serving the campus. We investigate the ``first phase'' sample, which were taken on a 100m equilateral triangular lattice.





\section{Conclusion}
First, different from the transformation of DNN, they can keep the topology structure of the space.  Second, It is not prone to overfit during the training procedual. However, the hyper-parameter is subttle.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{./bibtex/myIEEEtran.bst}
\bibliography{./bibtex/library}

%------------------------------------------------------------------------------------------------
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Yi-Qun_Xu}}]{Yi-Qun~Xu}
%   received the B.S. and M.S. degrees in information and communication
%   engineering from the PLA University of Science and Technology, Nanjing, China,
%   in 2005 and 2010, respectively. He is currently pursuing the Ph.D. degree with
%   Army Engineering University, Nanjing, China.

%   His current research interests include cognitive radio, spectrum sensing,
%   spatial-temporal data analysis, big data analysis, and machine learning.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Bangning_Zhang}}]{Bangning~Zhang}
%   received the B.S. and M.S. degrees from the Institute of Communications Engineering, Nanjing, China, in 1984 and 1987, respectively. His current research interests include microwave technologies, satellite communication systems, communication anti-jamming technologies, and physical layer security.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Guoru_Ding}}]{Guoru~Ding}
%   (S'10-M'14-SM'16) is currently an Associate Professor with the College of Communications Engineering, Nanjing, China. He received the B.S. (Hons.) degree in electrical engineering from Xidian University, Xi'an, China, in 2008, and the Ph.D. (Hons.) degree in communications and information systems from the College of Communications Engineering, Nanjing, China, in 2014. From 2015 to 2018, he was a Post-Doctoral Research Associate with the National Mobile Communications Research Laboratory, Southeast University, Nanjing, China. His research interests include cognitive radio networks, massive MIMO, machine learning, and data analytics over wireless networks.

%   He has received the Excellent Doctoral Thesis Award of the China Institute of Communications in 2016, the Alexander von Humboldt Fellowship in 2017, the Excellent Young Scientist of Wuwenjun Artificial Intelligence in 2018, and the 14th IEEE COMSOC Asia-Pacific Outstanding Young Researcher Award in 2019. He was a recipient of the Natural Science Foundation for Distinguished Young Scholars of Jiangsu Province, China and six best paper awards from international conferences such as the IEEE VTC-FALL 2014. He has served as a Guest Editor for the IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS (special issue on spectrum sharing and aggregation in future wireless networks). He is currently an Associate Editor of the IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, an Editor of CHINA COMMUNICATIONS and a Technical Editor of the IEEE 1900.6 Standard Association Working Group.
%   \end{IEEEbiography}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Bing_Zhao}}]{Bing~Zhao}
%   received the B.S. degree and M.S. degree from Nanjing University of Science and Technology (NJUST), Nanjing, China, in 2007 and 2009, respectively. She is currently a full lecturer with Army Engineering University of PLA. She has been granted over 10 patents in her research areas. Her current research interests include satellite communications systems and transmission technologies.
%   \end{IEEEbiography}

%   \begin{IEEEbiographynophoto}{Shengnan~Li}
%   received the B.S. degree from the Changsha University of Science and Technology, Changsha, China, in 2014, and the M.S. degree from the PLA University of Science and Technology, Nanjing, in 2017. Her research interests focus on spectrum sensing, satellite communications, and communication anti-jamming technologies.
%   \end{IEEEbiographynophoto}

%   \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{../author/Author_Daoxing_Guo}}]{Daoxing~Guo}
%   received the B.S., M.S., and Ph.D. degrees from the Institute of Communications Engineering, Nanjing, China, in 1995, 1999, and 2002, respectively. His current research interests include satellite communication systems and transmission technologies, communication anti-jamming technologies, and communication anti-interception technologies.
%   \end{IEEEbiography}
\end{document}